{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Homework 6\n\n\n## EMNIST Dataset\n\n[EMNIST dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset) is a set of hand-written characters and digits. Each of the data points is a grayscale image of size 28x28 pixels.  The structure of the dataset is the same as the infamous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, but this dataset contains more samples and also contains characters. You can find more information on the dataset in the paper available at [https://arxiv.org/abs/1702.05373v1](https://arxiv.org/abs/1702.05373v1)\n\nYou can find the dataset you need [at this link](https://www.dropbox.com/sh/vgap8ici7xs5w7f/AACE-9RrDpbGCc6bP72gHRfUa?dl=0).  Please download and use your local copy to do the homework.\n\n## Task 1\n\nIngest the data (both the train and test sets) into this pyhthon notebook as a numpy array.\n\n\n## Task 2\n\nWrite a convolutional artifial neural network model, train it and test it.\n\n\n## Notes\n\n1. You need to document each of your steps in both the ingestion phase and processing phase: explain the steps taken, the problems you encounter, how you solved them.\n\n2. DO NOT write python classes.  In other words, I do not want to see `__init__` or `__main__` in your code.  They are hard to follow (as they contain mutable state) and hard to port to future code you might write on a similar project.\n\n3. When you upload your solution to github, DO NOT include the datasets. They are large and I already have copies. I can test your models on the copy I have."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.data import loadlocal_mnist\nfrom sklearn.preprocessing import Normalizer\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Conv1D, Conv2D, Flatten, MaxPooling2D\nfrom keras.optimizers import Adam, Adamax, RMSprop, SGD\n\nimport keras.backend as K\nimport keras as keras",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task 1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train, y_train = loadlocal_mnist(images_path = '../data/EMNIST/emnist-balanced-train-images-idx3-ubyte', labels_path = '../data/EMNIST/emnist-balanced-train-labels-idx1-ubyte')\nx_test, y_test = loadlocal_mnist(images_path = '../data/EMNIST/emnist-balanced-test-images-idx3-ubyte', labels_path = '../data/EMNIST/emnist-balanced-test-labels-idx1-ubyte')",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task 2"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As said in the lecture, data fitting is a problem in EMNIST dataset. So we arranged our data as categorical."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_train = keras.utils.to_categorical(y_train)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Lets take a look at our data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_train",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "array([[0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now our dataset should be fine. Now lets try to implement it. We know that data size is large, we can see that from the length of the result; so it will take time."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(x_train)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "112800"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\n\nmodel.add(Conv2D(64, (4,4), input_shape=(28,28,1,), activation='tanh'))",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "I decided to use relu and sigmoid functions to construct model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.25))",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.add(MaxPooling2D(pool_size=(2,2)))",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.add(Flatten())\nmodel.add(Dense(47, activation='sigmoid'))\nmodel.add(Dropout(0.05))",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='RMSProp')\nmodel.fit(x_train.reshape(len(x_train),28,28,1), y_train, batch_size=512, epochs=10)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/10\n 55808/112800 [=============>................] - ETA: 489s - loss: 2.0791 - acc: 0.5789",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Taking tremendous amount of time to complete the calculation, as it is observable. Since lack of time to complete full calculation, I have to post it to GitHub before waiting for it to complete."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}